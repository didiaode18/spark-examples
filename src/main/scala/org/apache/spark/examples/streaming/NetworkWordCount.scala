/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.examples.streaming

import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.storage.StorageLevel

/**
 * --Howto--
 * Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
 *
 * Usage: NetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999`
 *
 * ----------------------------------------------
 *
 * --DStream--
 *
 * DStream represents a continuous stream of data,
 * either the input data stream received from source,
 * or the processed data stream generated by transforming the input stream
 *
 * a DStream is represented by a continuous series of RDDs 一个DStream代表了连续的RDD
 * 比如lines这个DStream,每一秒钟的数据就是一个RDD. 这样不断连续的流, 形成了RDD流. 所有这些每秒形成的RDD构成了一个DStream
 * Each RDD in a DStream contains data from a certain interval
 * DStream中的每个RDD包含了一段时间范围的数据, 比如一秒钟之内的数据
 *
 *              RDD@time1  RDD@time2   RDD@time3  RDD@time4
 * DSteram ----|----------|----------|----------|----------|-->
 *             data from   data from  data from  data from
 *             t0 to t1    t1 to t2   t2 to t3   t3 to t4
 *
 * --Receiver--
 *
 * Every input DStream is associated with a Receiver object which
 * receives the data from a source and stores it in Spark’s memory for processing.
 * 每个输入的DStream都关联了一个Receiver对象,从数据源中接收数据,保存到Spark内存中,用于处理
 * 除了运行Receiver的线程外,还要有处理接收到的数据的线程.
 *
 * a Spark worker/executor is a long-running task, hence it occupies one of the cores allocated to the Spark Streaming application
 * need to be allocated enough cores to process the received data, as well as to run the receiver(s).
 *
 * 在本地模式运行时,不要使用local或local[1],因为这意味着运行本地的任务只有一个线程.而这个线程只被用于Receiver,不会被用于处理数据
 * means that only one thread will be used for running tasks locally
 * If you are using a input DStream based on a receiver,    使用基于Receiver的输入DStream
 * then the single thread will be used to run the receiver, 这个唯一的线程会被用于运行这个Receiver
 * leaving no thread for processing the received data.      导致没有线程用于处理接收到的数据
 *
 * 在集群中运行Streamming时,集群的CPU数要比Receiver数要多,否则系统只会接收数据,不会处理数据.
 *
 */
object NetworkWordCount {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: NetworkWordCount <hostname> <port>")
      System.exit(1)
    }
    val (host,port) = (args(0), args(1).toInt)
    StreamingExamples.setStreamingLogLevels()

    // Create a local StreamingContext with two working thread and batch interval of 1 second.
    // The master requires 2 cores to prevent from a starvation scenario.

    // Create the context with a 1 second batch size
    val sparkConf = new SparkConf().setAppName("NetworkWordCount")
    //每隔一秒钟统计一次. 即统计过去一秒时间内的.
    //这里的时间指的是一个DStream每隔一秒形成一个RDD.这些RDD形成了DStream.
    //后面的window function和这个不一样: window function由于时间窗口>=1,一个时间窗口内的RDD不止一个!
    val ssc = new StreamingContext(sparkConf, Seconds(1))

    // Create a socket stream(DStream) on target ip:port and count the words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.

    //create a DStream that represents streaming data from a TCP source,
    //This lines DStream represents the stream of data that will be received from the data server(socket)
    //Each record in this DStream is a line of text
    val lines = ssc.socketTextStream(host, port, StorageLevel.MEMORY_AND_DISK_SER)
    //flatMap is a one-to-many DStream operation that  creates a new DStream
    //by generating multiple new records from each record in the source DStream
    //源DStream中(lines)的每条记录,通过flatMap会创建一个新的DStream(words), 会生成多条记录
    //现在words这个DStream中的每条记录是一个个单词.
    val words = lines.flatMap(_.split(" "))
    // Count each word in each batch: The words DStream is further mapped (one-to-one transformation) to a DStream of (word, 1) pairs
    val pairs = words.map(word => (word, 1))
    //reduced to get the frequency of words in each batch of data
    val wordCounts = pairs.reduceByKey(_ + _)

    //------------------
    // Window Operations
    // Reduce last 30 seconds of data(windoweLength 时间窗口长度), every 10 seconds(slidingInterval 滑动间隔)
    val windowedWordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) => (a + b), Seconds(30), Seconds(10))
    val windowedStream1 = lines.window(Seconds(30))
    val windowedStream2 = lines.window(Seconds(10))
    //val joinedStream = windowedStream1.join(windowedStream2)

    //------------------

    // Print the first ten elements of each RDD generated in this DStream(generated every second) to the console
    wordCounts.print()
    ssc.start()             // Start the computation
    ssc.awaitTermination()  // Wait for the computation to terminate
  }
}
