/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.examples.streaming

import org.apache.spark.SparkConf
import org.apache.spark.HashPartitioner
import org.apache.spark.streaming._

/**
 * 演示: 有状态的单词计数.
 * 前面的例子是过去一秒的统计,现在要统计过去所有数据的统计.
 *
 * Counts words cumulatively in UTF8 encoded, '\n' delimited text received from the network every
 * second starting with initial value of word count.
 * Usage: StatefulNetworkWordCount <hostname> <port>
 *   <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive
 *   data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example
 *      org.apache.spark.examples.streaming.StatefulNetworkWordCount localhost 9999`
 */
object StatefulNetworkWordCount {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: StatefulNetworkWordCount <hostname> <port>")
      System.exit(1)
    }
    StreamingExamples.setStreamingLogLevels()

    val sparkConf = new SparkConf().setAppName("StatefulNetworkWordCount")
    // Create the context with a 1 second batch size
    //batch size大小, 表示每隔多少秒统计一次数据. 可以理解为滑动窗口.
    //对于无状态的, 时间窗口为1秒, 有状态的, 时间窗口是无限的.
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    ssc.checkpoint(".")

    // Initial RDD input to updateStateByKey
    val initialRDD = ssc.sparkContext.parallelize(List(("hello", 1), ("world", 1)))

    // Create a ReceiverInputDStream on target ip:port and count the
    // words in input stream of \n delimited test (eg. generated by 'nc')
    val lines = ssc.socketTextStream(args(0), args(1).toInt)
    val words = lines.flatMap(_.split(" "))
    //word DStream的结果是(单词,1)的组合.
    //没有状态的单词计数是在本次时间窗口内(1s),reduceByKey(_+_), 表示每隔一秒统计过去一秒内的单词计数
    //而有状态的单词计数,指定的时间窗口为1秒,表示每隔一秒统计过去所有时间段内的单词计数.
    val wordDstream = words.map(x => (x, 1))

    //----------------------------------------------------------------------
    //reduceByKey不需要处理key,我们只需要对根据key分组后的value集合处理即可比如_+_
    //updateStateByKey对于有状态的, 存在旧的状态和新的数据. 比如旧的状态值为(word,10)
    //新的数据为(word,1),(word,1), 则要更新为(word,12)
    //updateStateByKey的含义就是key相同(这里是word)都会被一起处理
    //所以newUpdateFunc的参数iterator的每个元素. [1]:String=word, [2]:Seq[Int]=(1,1), [3]:Option[Int]=10
    //Seq[Int]=(1,1)和Option[Int]=10作为updateFunc的values和state分别表示新的数据集和旧的结果值
    //最后updateFunc的计算结果为10+1+1=14.  newUpdateFunc的返回值为(word,12)
    //----------------------------------------------------------------------

    //针对一个单词的Update函数
    //val runningCounts = wordDstream.updateStateByKey[Int](updateFunc _)  //ERROR
    //val runningCounts2 = wordDstream.updateStateByKey[Int](updateFunc)   //OK
    //wordDstream.updateStateByKey[Int](newUpdateFunc)                     //ERROR

    // Update the cumulative count using updateStateByKey
    // This will give a Dstream made of state (which is the cumulative count of the words)
    val stateDstream = wordDstream.updateStateByKey[Int](newUpdateFunc,
      new HashPartitioner(ssc.sparkContext.defaultParallelism), true, initialRDD)

    stateDstream.print()

    ssc.start()
    ssc.awaitTermination()
  }

  /**
   * 更新函数
   * values: 本次传入的集合
   * state: 旧的值(计算结果,而非集合)
   *
   * The update function will be called for each word, with newValues having a sequence of 1’s
   * (from the (word, 1) pairs) and the runningCount having the previous count.
   */
  val updateFunc = (values: Seq[Int], state: Option[Int]) => {
    val currentCount = values.sum
    val previousCount = state.getOrElse(0)
    // add the new values with the previous running count to get the new count
    Some(currentCount + previousCount)
  }

  /**
   * 这里的参数是一个迭代器,即集合. 而上面的updateFunc是针对一个元素而言.
   * 比如在一次时间窗口内, 数据有(word,1),(word,1),(hello,1)
   * 则iterator有两个元素: (word,Seq(1,1),..), (hello,Seq(1),..)
   */
  val newUpdateFunc = (iterator: Iterator[(String, Seq[Int], Option[Int])]) => {
    iterator.flatMap(t => updateFunc(t._2, t._3).map(s => (t._1, s)))
  }
  //----------------------------------------------------------------------
}
